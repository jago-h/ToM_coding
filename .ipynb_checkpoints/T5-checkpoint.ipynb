{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df646c-127b-4a29-980c-251eeb458767",
   "metadata": {},
   "source": [
    "# Install all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84852136-115c-436f-b7c8-ab9fcb932301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.11/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.11/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from keras) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.11/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install transformers datasets\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0c4bf8-6828-42b0-951d-d3d1bd16eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Word Cloud\n",
    "from wordcloud import WordCloud\n",
    "# from textacy import preprocessing\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0fe7a-65b0-4fb7-839a-06f3129b92f0",
   "metadata": {},
   "source": [
    "# Feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a6a709-011a-472e-a245-3a252af5d93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>cog_self</th>\n",
       "      <th>aff_self</th>\n",
       "      <th>int_self</th>\n",
       "      <th>cog_target</th>\n",
       "      <th>aff_target</th>\n",
       "      <th>int_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two children are stealing cookies from the jar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This house seems chaotic. The boy is on a stoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Presumably)Mother daydreaming, distracted by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a woman is washing the dishes\\nwater is spilli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A woman is washing the plates.However, the wat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Q  cog_self  aff_self  \\\n",
       "0  two children are stealing cookies from the jar...         0         0   \n",
       "1  This house seems chaotic. The boy is on a stoo...         0         0   \n",
       "2  (Presumably)Mother daydreaming, distracted by ...         0         0   \n",
       "3  a woman is washing the dishes\\nwater is spilli...         0         0   \n",
       "4  A woman is washing the plates.However, the wat...         0         0   \n",
       "\n",
       "   int_self  cog_target  aff_target  int_target  \n",
       "0         0           0           0           1  \n",
       "1         0           1           0           0  \n",
       "2         0           2           0           0  \n",
       "3         0           0           0           0  \n",
       "4         0           1           0           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path1 = \"5vec2.csv\"\n",
    "file_path2 = \"watch_closely.csv\"\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4253d473-ab8d-49fc-ab86-a6c079573589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>aff_tar</th>\n",
       "      <th>cog_tar</th>\n",
       "      <th>int_tar</th>\n",
       "      <th>aff_oth</th>\n",
       "      <th>cog_oth</th>\n",
       "      <th>int_oth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A child and a man, presumably the father, are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scene opens with a father and daughter riding ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A father and daughter rode their bikes along a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The clip commences with a cross view of the gi...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A girl cycles with her father to see him off a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Q aff_tar cog_tar int_tar  \\\n",
       "0  A child and a man, presumably the father, are ...       1       0       0   \n",
       "1  Scene opens with a father and daughter riding ...       1       1       0   \n",
       "2  A father and daughter rode their bikes along a...       0       0       0   \n",
       "3  The clip commences with a cross view of the gi...       0       3       1   \n",
       "4  A girl cycles with her father to see him off a...       0       1       1   \n",
       "\n",
       "  aff_oth cog_oth int_oth  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       0       0  \n",
       "3       1       0       0  \n",
       "4       0       0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae67d5-98d8-4e76-9d56-02548f64a2f3",
   "metadata": {},
   "source": [
    "# Split data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f601dfc4-9878-4824-bab0-bf5ff82ccdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is in a pandas DataFrame called 'data'\n",
    "train_df, temp_data = train_test_split(df1, test_size=0.25, random_state=42)\n",
    "# Split the temp_data into validation (15%) and test (10%)\n",
    "valid_df, test_df = train_test_split(temp_data, test_size=0.4, random_state=42)  # 0.4 * 25% = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8eddceda-bae6-4b36-9af9-d24e4a93c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 236 rows\n",
      "Validation set size: 47 rows\n",
      "Test set size: 32 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train_df)} rows\")\n",
    "print(f\"Validation set size: {len(valid_df)} rows\")\n",
    "print(f\"Test set size: {len(test_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3509303-b62f-4174-a7c7-52840e2c0e66",
   "metadata": {},
   "source": [
    "# Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076c067c-86a1-4e9d-8804-af7a09b9aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset \n",
    "from transformers import AdamW\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bf8335d-fcb7-44c5-ba96-8bcdaebaa3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset Class\n",
    "class ToMDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=1775):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # Add task-specific instructions for training\n",
    "        text = row['Q']\n",
    "        target = f\"{row['cog_self']} {row['aff_self']} {row['int_self']} {row['cog_target']} {row['aff_target']} {row['int_target']}\"\n",
    "\n",
    "        # Tokenize input and target\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            target, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=6, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs.input_ids.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'target_ids': targets.input_ids.squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "977dc107-2b47-409e-a0c5-939195994d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = ToMDataset(train_df, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f36ad34-70ee-40b8-b1c9-8829c941fbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Use a GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac31046-e197-4014-899b-bc81e3b6c5e3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "094c2f97-7606-4510-954a-ab6e08d9042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Loss: 7.077762126922607\n",
      "Loss: 6.716832637786865\n",
      "Loss: 5.908145427703857\n",
      "Loss: 6.1862053871154785\n",
      "Loss: 4.862710952758789\n",
      "Loss: 5.549628734588623\n",
      "Loss: 4.801024913787842\n",
      "Loss: 5.030360221862793\n",
      "Loss: 4.753163814544678\n",
      "Loss: 4.195201396942139\n",
      "Loss: 4.325935363769531\n",
      "Loss: 4.023024082183838\n",
      "Loss: 4.245853900909424\n",
      "Loss: 3.8592069149017334\n",
      "Loss: 3.91497802734375\n",
      "Loss: 3.340871810913086\n",
      "Loss: 3.647738218307495\n",
      "Loss: 3.699476480484009\n",
      "Loss: 4.416460990905762\n",
      "Loss: 3.814748764038086\n",
      "Loss: 3.2606146335601807\n",
      "Loss: 3.2982637882232666\n",
      "Loss: 3.4785473346710205\n",
      "Loss: 3.6264684200286865\n",
      "Loss: 3.0236940383911133\n",
      "Loss: 3.5930445194244385\n",
      "Loss: 2.970031499862671\n",
      "Loss: 3.417034149169922\n",
      "Loss: 3.306366205215454\n",
      "Loss: 4.284363269805908\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Number of training steps\n",
    "num_epochs = 1 # I start with 1 coz it takes some time and firstly we need to make it work, then we can do more epochs\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,  # Default value\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Move data to device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=target_ids\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Print loss for monitoring\n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eaae8357-fb47-46b9-8bc8-b9b70f8a4327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_toM_count_model/tokenizer_config.json',\n",
       " './t5_toM_count_model/special_tokens_map.json',\n",
       " './t5_toM_count_model/spiece.model',\n",
       " './t5_toM_count_model/added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./t5_toM_count_model\")\n",
    "tokenizer.save_pretrained(\"./t5_toM_count_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969b89a-f683-4df8-a7c0-9a89325b13a9",
   "metadata": {},
   "source": [
    "# Evaluate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c42a5430-3be1-42e2-972f-1434dc2c340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_toM_count_model\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5_toM_count_model\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Use a GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dd41903-df5e-41ea-8add-6e357a4d1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   27,   816,    34,  ...,     0,     0,     0],\n",
      "        [  255,    47, 11237,  ...,     0,     0,     0],\n",
      "        [ 1029,   160,  3503,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  499,  4284,    19,  ...,     0,     0,     0],\n",
      "        [   27,    47,   578,  ...,     0,     0,     0],\n",
      "        [ 1029,  4496,    31,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'target_ids': tensor([[  3, 632,   3, 632,   3,   1],\n",
      "        [  3, 632,   3, 632,   3,   1],\n",
      "        [  3, 632,   3, 632,   3,   1],\n",
      "        [209,   3, 632,   3, 632,   1],\n",
      "        [314,   3, 632,   3, 632,   1],\n",
      "        [  3, 632,   3, 632,   3,   1],\n",
      "        [  3, 632,   3, 632,   3,   1],\n",
      "        [  3, 632,   3, 632,   3,   1]])}\n"
     ]
    }
   ],
   "source": [
    "# Now create the ToMDataset instance\n",
    "test_data = ToMDataset(test_df, tokenizer)\n",
    "\n",
    "# Create a DataLoader for the test data\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "\n",
    "# Check the first batch\n",
    "for batch in test_loader:\n",
    "    print(batch)  # This should work without an error\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fe21f-4fe9-436c-a166-158320f1b899",
   "metadata": {},
   "source": [
    "# Check predicted and true values\n",
    "## Here we have some **issues**:  \n",
    "firstly - some sentences have true labels as only 2 numbers, not 3;  \n",
    "secondly - our predicted categories are some random pieces of text...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7c525b4-ade8-4e18-8c6a-0b518ae4dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['I thought it was very', 'Elle ne', 'perspective, she was', 'Sarah. Sarah had', 'the scenario, I', 'is sneaking me', '. She would look', 'Jack was sitting directly across']\n",
      "True: ['0 0', '0 0', '0 0', '1 0 0', '4 0 0', '0 0', '0 0', '0 0']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids, max_length=6)\n",
    "\n",
    "        # Decode the predictions and targets\n",
    "        predicted_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        true_text = [tokenizer.decode(target, skip_special_tokens=True) for target in target_ids]\n",
    "\n",
    "        # Print the predictions and true values for debugging\n",
    "        print(f\"Predicted: {predicted_text}\")\n",
    "        print(f\"True: {true_text}\")\n",
    "        break  # Exit after one batch to examine the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4f963-52ec-42c7-bb03-f4dbeb9f78af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
